{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Cleaned Data.\n",
    "- Split the Data into training, validation, and prediction datasets.\n",
    "- Perform Feature Engineering.\n",
    "- Train and Evaluate the Model.\n",
    "- Train the Final Model using the entire training dataset.\n",
    "- Predict the completed column for the null values.\n",
    "- Integrate Predictions back into the main DataFrame.\n",
    "- Save the Final DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    return SparkSession.builder.appName(\"JiraMetricsModeling\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cleaned_data(spark: SparkSession, file_path: str) -> DataFrame:\n",
    "    # Load the cleaned data from parquet\n",
    "    df = spark.read.parquet(file_path)\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df: DataFrame) -> DataFrame:\n",
    "    # Extract date features\n",
    "    df = df.withColumn(\"year\", F.year(\"date\")).withColumn(\"month\", F.month(\"date\"))\n",
    "\n",
    "    # StringIndexer for categorical columns\n",
    "    engineer_indexer = StringIndexer(inputCol=\"engineer\", outputCol=\"engineer_index\")\n",
    "    initiative_indexer = StringIndexer(inputCol=\"initiative\", outputCol=\"initiative_index\")\n",
    "    repo_name_indexer = StringIndexer(inputCol=\"repo_name\", outputCol=\"repo_name_index\")\n",
    "\n",
    "    # OneHotEncoder for indexed columns\n",
    "    engineer_encoder = OneHotEncoder(inputCol=\"engineer_index\", outputCol=\"engineer_vec\")\n",
    "    initiative_encoder = OneHotEncoder(inputCol=\"initiative_index\", outputCol=\"initiative_vec\")\n",
    "    repo_name_encoder = OneHotEncoder(inputCol=\"repo_name_index\", outputCol=\"repo_name_vec\")\n",
    "\n",
    "    # VectorAssembler to combine feature columns\n",
    "    assembler = VectorAssembler(inputCols=[\"num_slack_messages\", \"num_hours\", \"engineer_vec\", \"initiative_vec\", \"repo_name_vec\", \"year\", \"month\"], outputCol=\"features\")\n",
    "\n",
    "    # Pipeline to streamline feature engineering\n",
    "    pipeline = Pipeline(stages=[engineer_indexer, initiative_indexer, repo_name_indexer, engineer_encoder, initiative_encoder, repo_name_encoder, assembler])\n",
    "\n",
    "    # Fit and transform the DataFrame\n",
    "    model = pipeline.fit(df)\n",
    "    df = model.transform(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def train_and_evaluate_model(df: DataFrame) -> Pipeline:\n",
    "    # Filter out rows where 'completed' is not null\n",
    "    train_df = df.filter(df.completed.isNotNull())\n",
    "\n",
    "    # Ensure the 'completed' column is strictly binary\n",
    "    train_df = train_df.withColumn(\"completed_str\", F.col(\"completed\").cast(\"string\"))\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    train_data, val_data = train_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # StringIndexer for the label column\n",
    "    label_indexer = StringIndexer(inputCol=\"completed_str\", outputCol=\"label\")\n",
    "\n",
    "    # RandomForestClassifier\n",
    "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "    # Use TrainValidationSplit for hyperparameter tuning\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [20, 50]) \\\n",
    "        .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "        .build()\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "    tvs = TrainValidationSplit(estimator=Pipeline(stages=[label_indexer, rf]),\n",
    "                               estimatorParamMaps=param_grid,\n",
    "                               evaluator=evaluator,\n",
    "                               trainRatio=0.8)\n",
    "\n",
    "    # Train the model using TrainValidationSplit\n",
    "    model = tvs.fit(train_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = model.transform(val_data)\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    print(f\"Validation AUC: {auc}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_completed(df: DataFrame, model: Pipeline) -> DataFrame:\n",
    "    # Filter out rows where 'completed' is null\n",
    "    test_df = df.filter(df.completed.isNull())\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # Select the relevant columns\n",
    "    predictions = predictions.select(\"jira_ticket_id\", \"prediction\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def integrate_predictions(df: DataFrame, predictions: DataFrame) -> DataFrame:\n",
    "    # Rename prediction column to completed\n",
    "    predictions = predictions.withColumnRenamed(\"prediction\", \"completed_predicted\")\n",
    "\n",
    "    # Join the predictions back to the main DataFrame\n",
    "    df = df.join(predictions, on=\"jira_ticket_id\", how=\"left\")\n",
    "\n",
    "    # Cast completed_predicted to boolean\n",
    "    df = df.withColumn(\"completed_predicted\", F.col(\"completed_predicted\").cast(\"boolean\"))\n",
    "\n",
    "    # If completed is null, use the predicted value\n",
    "    df = df.withColumn(\"completed\", F.when(df.completed.isNull(), df.completed_predicted).otherwise(df.completed))\n",
    "\n",
    "    # Drop the temporary prediction column\n",
    "    df = df.drop(\"completed_predicted\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "df = load_cleaned_data(spark, \"../data/cleaned_data.parquet\")\n",
    "df = feature_engineering(df)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = train_and_evaluate_model(df)\n",
    "\n",
    "# Predict 'completed' for null values\n",
    "predictions = predict_completed(df, model)\n",
    "\n",
    "# Integrate the predictions back into the main DataFrame\n",
    "df = integrate_predictions(df, predictions)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DataFrame to parquet\n",
    "#drop the cols [year|month|engineer_index|initiative_index|repo_name_index|engineer_vec|initiative_vec|repo_name_vec|features]\n",
    "df = df.drop(\"year\", \"month\", \"engineer_index\", \"initiative_index\", \"repo_name_index\", \"engineer_vec\", \"initiative_vec\", \"repo_name_vec\", \"features\")\n",
    "df.write.mode(\"overwrite\").parquet(\"../data/predicted_datav0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+------------------+------------------+--------+--------------------+------------+-----------+---------+-----------+\n",
      "|jira_ticket_id|      date|completed|num_slack_messages|         num_hours|engineer|  ticket_description|  initiative|new_revenue|repo_name|lines_added|\n",
      "+--------------+----------+---------+------------------+------------------+--------+--------------------+------------+-----------+---------+-----------+\n",
      "|             1|2023-03-31|     true|             276.0| 62.13453674316406|    Dale|Taurus both absen...|  Efficiency|  5295.6553|        G|         42|\n",
      "|            12|2023-12-30|     true|             436.0| 61.72333526611328|   Daisy|Toronto funereal ...|  Efficiency|  3425.6223|        Q|         73|\n",
      "|            13|2023-03-24|    false|             294.0|  97.2278060913086| Unknown|sizzle animism ed...|  Efficiency|   2806.764|        R|         22|\n",
      "|            22|2023-06-22|    false|             366.0| 45.48678970336914|    Josh|spitfire aorta ji...|New Customer|   9540.159|        K|         63|\n",
      "|            47|2023-02-02|    false|             148.0|39.095054626464844|    Dale|structure immersi...|     Support|   5668.918|        W|         35|\n",
      "+--------------+----------+---------+------------------+------------------+--------+--------------------+------------+-----------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
