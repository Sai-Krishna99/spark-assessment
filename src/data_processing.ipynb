{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data:\n",
    "- Loads the data with the defined schema.\n",
    "\n",
    "#### Explode Nested Columns:\n",
    "- Explodes the KPIs column and flattens it.\n",
    "- Explodes the lines_per_repo column and flattens it, extracting the keys and values from the map.\n",
    "\n",
    "#### Clean Data:\n",
    "- Imputes missing dates with January 1, 2024 as the distribution seems to fit that date better.\n",
    "- Converts the completed column to Boolean.\n",
    "- Replaces nulls in engineer with \"Unknown\".\n",
    "- Imputes missing num_slack_messages and num_hours with their respective medians after checking summary statitics.\n",
    "- Handles missing or invalid values in ticket_description, initiative, new_revenue, repo_name, and lines_added.\n",
    "- Ensures jira_ticket_id is unique.\n",
    "- Validates num_hours and num_slack_messages to ensure they are non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    return SparkSession.builder.appName(\"JiraMetrics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spark: SparkSession, file_path: str) -> DataFrame:\n",
    "    # Define the expected schema\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"jira_ticket_id\", T.IntegerType(), True),\n",
    "        T.StructField(\"date\", T.DateType(), True),\n",
    "        T.StructField(\"completed\", T.StringType(), True),  # Initially as StringType to handle \"yes\", \"True\", \"None\"\n",
    "        T.StructField(\"num_slack_messages\", T.IntegerType(), True),\n",
    "        T.StructField(\"num_hours\", T.FloatType(), True),\n",
    "        T.StructField(\"engineer\", T.StringType(), True),\n",
    "        T.StructField(\"ticket_description\", T.StringType(), True),\n",
    "        T.StructField(\"KPIs\", T.ArrayType(\n",
    "            T.StructType([\n",
    "                T.StructField(\"initiative\", T.StringType(), True),\n",
    "                T.StructField(\"new_revenue\", T.FloatType(), True)\n",
    "            ])\n",
    "        ), True),\n",
    "        T.StructField(\"lines_per_repo\", T.ArrayType(\n",
    "            T.MapType(T.StringType(), T.IntegerType())\n",
    "        ), True)\n",
    "    ])\n",
    "\n",
    "    # Load the JSON data into a DataFrame\n",
    "    df = spark.read.schema(schema).json(file_path)\n",
    "\n",
    "    return df\n",
    "\n",
    "def analyze_null_distribution(df: DataFrame):\n",
    "    # Count nulls in each column\n",
    "    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+---------+------------------+---------+--------+------------------+----+--------------+\n",
      "|jira_ticket_id|date|completed|num_slack_messages|num_hours|engineer|ticket_description|KPIs|lines_per_repo|\n",
      "+--------------+----+---------+------------------+---------+--------+------------------+----+--------------+\n",
      "|             0|1970|    14285|                 0|        0|   16315|                 0|   0|             0|\n",
      "+--------------+----+---------+------------------+---------+--------+------------------+----+--------------+\n",
      "\n",
      "+--------------+----------+---------+------------------+---------+--------+--------------------+--------------------+--------------------+\n",
      "|jira_ticket_id|      date|completed|num_slack_messages|num_hours|engineer|  ticket_description|                KPIs|      lines_per_repo|\n",
      "+--------------+----------+---------+------------------+---------+--------+--------------------+--------------------+--------------------+\n",
      "|             0|2023-03-18|      yes|               237|22.898462|  Sandra|reclusive initiat...|[{New Customer, 4...|[{G -> 52}, {O ->...|\n",
      "|             1|2023-03-31|     true|               276|62.134537|    Dale|Taurus both absen...|[{Efficiency, 529...|[{G -> 42}, {S ->...|\n",
      "|             2|2023-07-09|     NULL|                72|12.258103|    Josh|precess canister ...|[{Support, 1034.0...|[{F -> 14}, {M ->...|\n",
      "|             3|2023-12-10|     true|               371|39.005272|    Dale|Oldenburg Stalin ...|[{New Customer, 9...|[{A -> 64}, {Y ->...|\n",
      "|             4|2023-04-29|     true|               217|80.074425|    Dale|tasting prostitut...|[{New Customer, 7...|         [{T -> 22}]|\n",
      "+--------------+----------+---------+------------------+---------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "df = load_data(spark, \"../data/data.json\")\n",
    "analyze_null_distribution(df)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_nested_columns(df: DataFrame):\n",
    "    # Explode KPIs column and inspect nested fields\n",
    "    print(\"Exploring KPIs column:\")\n",
    "    df.select(F.col(\"jira_ticket_id\"), F.explode(\"KPIs\").alias(\"KPI\")).select(\"jira_ticket_id\", \"KPI.*\").show()\n",
    "\n",
    "    # Explode lines_per_repo column and inspect nested fields\n",
    "    print(\"Exploring lines_per_repo column:\")\n",
    "    df.select(F.col(\"jira_ticket_id\"), F.explode(\"lines_per_repo\").alias(\"repo\")) \\\n",
    "      .select(\"jira_ticket_id\", F.expr(\"map_keys(repo)[0]\").alias(\"repo_name\"), F.expr(\"map_values(repo)[0]\").alias(\"lines_added\")) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring KPIs column:\n",
      "+--------------+------------+-----------+\n",
      "|jira_ticket_id|  initiative|new_revenue|\n",
      "+--------------+------------+-----------+\n",
      "|             0|New Customer|  4349.9053|\n",
      "|             1|  Efficiency|  5295.6553|\n",
      "|             1|New Customer|   4360.982|\n",
      "|             2|     Support|  1034.0549|\n",
      "|             2|     Support|  6768.2495|\n",
      "|             3|New Customer|   9062.461|\n",
      "|             3|     Support|  7093.9927|\n",
      "|             4|New Customer|  782.30023|\n",
      "|             4|  Efficiency|   2014.079|\n",
      "|             5|New Customer|  503.27695|\n",
      "|             5|New Customer|  2257.7773|\n",
      "|             6|New Customer|   3002.858|\n",
      "|             7|  Efficiency|   3005.427|\n",
      "|             7|     Support|  7779.5337|\n",
      "|             8|  Efficiency|  5770.9575|\n",
      "|             8|  Efficiency|  268.91583|\n",
      "|             9|  Efficiency|  3023.7705|\n",
      "|            10|New Customer|   5879.525|\n",
      "|            11|New Customer|  1704.4084|\n",
      "|            12|  Efficiency|  3425.6223|\n",
      "+--------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Exploring lines_per_repo column:\n",
      "+--------------+---------+-----------+\n",
      "|jira_ticket_id|repo_name|lines_added|\n",
      "+--------------+---------+-----------+\n",
      "|             0|        G|         52|\n",
      "|             0|        O|         23|\n",
      "|             0|        H|         31|\n",
      "|             1|        G|         42|\n",
      "|             1|        S|         15|\n",
      "|             2|        F|         14|\n",
      "|             2|        M|         78|\n",
      "|             2|        C|         14|\n",
      "|             3|        A|         64|\n",
      "|             3|        Y|          6|\n",
      "|             3|        E|         50|\n",
      "|             3|        Q|         68|\n",
      "|             4|        T|         22|\n",
      "|             5|        J|         11|\n",
      "|             5|        E|         47|\n",
      "|             5|        I|         27|\n",
      "|             5|        Z|         39|\n",
      "|             6|        S|         25|\n",
      "|             6|        A|          5|\n",
      "|             7|        M|         81|\n",
      "+--------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_nested_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_nested_columns(df: DataFrame) -> DataFrame:\n",
    "    # Explode KPIs column\n",
    "    df = df.withColumn(\"KPIs_exploded\", F.explode_outer(\"KPIs\")) \\\n",
    "           .select(\"*\", \"KPIs_exploded.*\").drop(\"KPIs\", \"KPIs_exploded\")\n",
    "\n",
    "    # Explode lines_per_repo column\n",
    "    df = df.withColumn(\"lines_per_repo_exploded\", F.explode_outer(\"lines_per_repo\")) \\\n",
    "           .select(\"*\", F.expr(\"map_keys(lines_per_repo_exploded)[0]\").alias(\"repo_name\"),\n",
    "                   F.expr(\"map_values(lines_per_repo_exploded)[0]\").alias(\"lines_added\")) \\\n",
    "           .drop(\"lines_per_repo\", \"lines_per_repo_exploded\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+------------------+---------+--------+--------------------+------------+-----------+---------+-----------+\n",
      "|jira_ticket_id|      date|completed|num_slack_messages|num_hours|engineer|  ticket_description|  initiative|new_revenue|repo_name|lines_added|\n",
      "+--------------+----------+---------+------------------+---------+--------+--------------------+------------+-----------+---------+-----------+\n",
      "|             0|2023-03-18|      yes|               237|22.898462|  Sandra|reclusive initiat...|New Customer|  4349.9053|        G|         52|\n",
      "|             0|2023-03-18|      yes|               237|22.898462|  Sandra|reclusive initiat...|New Customer|  4349.9053|        O|         23|\n",
      "|             0|2023-03-18|      yes|               237|22.898462|  Sandra|reclusive initiat...|New Customer|  4349.9053|        H|         31|\n",
      "|             1|2023-03-31|     true|               276|62.134537|    Dale|Taurus both absen...|  Efficiency|  5295.6553|        G|         42|\n",
      "|             1|2023-03-31|     true|               276|62.134537|    Dale|Taurus both absen...|  Efficiency|  5295.6553|        S|         15|\n",
      "+--------------+----------+---------+------------------+---------+--------+--------------------+------------+-----------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_df = explode_nested_columns(df)\n",
    "exploded_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|year|month|count|\n",
      "+----+-----+-----+\n",
      "|NULL| NULL| 8941|\n",
      "|2023|    1|35608|\n",
      "|2023|    2|34066|\n",
      "|2023|    3|37320|\n",
      "|2023|    4|36710|\n",
      "|2023|    5|37308|\n",
      "|2023|    6|36706|\n",
      "|2023|    7|36778|\n",
      "|2023|    8|37064|\n",
      "|2023|    9|35987|\n",
      "|2023|   10|37625|\n",
      "|2023|   11|35667|\n",
      "|2023|   12|37903|\n",
      "|2024|    1| 1188|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze the date distribution\n",
    "date_distribution = exploded_df.groupBy(F.year(\"date\").alias(\"year\"), F.month(\"date\").alias(\"month\")).count().orderBy(\"year\", \"month\")\n",
    "date_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|num_slack_messages|\n",
      "+-------+------------------+\n",
      "|  count|            448871|\n",
      "|   mean| 250.4349044603015|\n",
      "| stddev|144.68258453512533|\n",
      "|    min|                 1|\n",
      "|    25%|               125|\n",
      "|    50%|               251|\n",
      "|    75%|               376|\n",
      "|    max|               500|\n",
      "+-------+------------------+\n",
      "\n",
      "Median of num_slack_messages: 255.0\n",
      "+-------+------------------+\n",
      "|summary|         num_hours|\n",
      "+-------+------------------+\n",
      "|  count|            448871|\n",
      "|   mean|48.926089710604224|\n",
      "| stddev|30.569540820445347|\n",
      "|    min|        -99.794624|\n",
      "|    25%|          24.27398|\n",
      "|    50%|         49.350273|\n",
      "|    75%|          74.60691|\n",
      "|    max|          99.99668|\n",
      "+-------+------------------+\n",
      "\n",
      "Median of num_hours: 48.76472473144531\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def get_summary_statistics(df: DataFrame, column: str):\n",
    "    # Describe the column to get summary statistics\n",
    "    summary = df.select(column).summary(\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\").show()\n",
    "\n",
    "    # Calculate the median using the approximate quantile method\n",
    "    median = df.approxQuantile(column, [0.5], 0.01)[0]\n",
    "    print(f\"Median of {column}: {median}\")\n",
    "\n",
    "# Get summary statistics\n",
    "get_summary_statistics(exploded_df, \"num_slack_messages\")\n",
    "get_summary_statistics(exploded_df, \"num_hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: DataFrame) -> DataFrame:\n",
    "    # Impute missing dates with January 1, 2024\n",
    "    df = df.withColumn(\"date\", F.when(F.col(\"date\").isNull(), F.lit(\"2024-01-01\")).otherwise(F.col(\"date\")))\n",
    "\n",
    "    # Convert 'completed' field to BooleanType\n",
    "    df = df.withColumn(\"completed\", F.when(F.col(\"completed\") == \"yes\", True)\n",
    "                                      .when(F.col(\"completed\") == \"True\", True)\n",
    "                                      .when(F.col(\"completed\") == \"None\", False)\n",
    "                                      .otherwise(F.col(\"completed\").cast(T.BooleanType())))\n",
    "\n",
    "    # Replace nulls in 'engineer' with 'Unknown'\n",
    "    df = df.withColumn(\"engineer\", F.when(F.col(\"engineer\").isNull(), \"Unknown\").otherwise(F.col(\"engineer\")))\n",
    "\n",
    "    # Impute missing 'num_slack_messages' with median\n",
    "    median_slack_messages = df.approxQuantile(\"num_slack_messages\", [0.5], 0.01)[0]\n",
    "    df = df.withColumn(\"num_slack_messages\", F.when(F.col(\"num_slack_messages\").isNull(), median_slack_messages).otherwise(F.col(\"num_slack_messages\")))\n",
    "\n",
    "    # Impute missing 'num_hours' with median and correct negative values\n",
    "    median_num_hours = df.approxQuantile(\"num_hours\", [0.5], 0.01)[0]\n",
    "    df = df.withColumn(\"num_hours\", F.when((F.col(\"num_hours\").isNull()) | (F.col(\"num_hours\") < 0), median_num_hours).otherwise(F.col(\"num_hours\")))\n",
    "\n",
    "    # Handling missing or invalid 'ticket_description'\n",
    "    df = df.withColumn(\"ticket_description\", F.when(F.col(\"ticket_description\").isNull(), \"No Description\").otherwise(F.col(\"ticket_description\")))\n",
    "\n",
    "    # Handling missing or invalid 'initiative' and 'new_revenue' in exploded KPIs\n",
    "    df = df.withColumn(\"initiative\", F.when(F.col(\"initiative\").isNull(), \"No Initiative\").otherwise(F.col(\"initiative\")))\n",
    "    df = df.withColumn(\"new_revenue\", F.when(F.col(\"new_revenue\").isNull(), 0).otherwise(F.col(\"new_revenue\")))\n",
    "\n",
    "    # Handling missing or invalid 'repo_name' and 'lines_added' in exploded lines_per_repo\n",
    "    df = df.withColumn(\"repo_name\", F.when(F.col(\"repo_name\").isNull(), \"Unknown\").otherwise(F.col(\"repo_name\")))\n",
    "    df = df.withColumn(\"lines_added\", F.when(F.col(\"lines_added\").isNull(), 0).otherwise(F.col(\"lines_added\")))\n",
    "\n",
    "    # Ensure unique jira_ticket_id\n",
    "    df = df.dropDuplicates([\"jira_ticket_id\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+------------------+-----------------+--------+--------------------+------------+-----------+---------+-----------+\n",
      "|jira_ticket_id|      date|completed|num_slack_messages|        num_hours|engineer|  ticket_description|  initiative|new_revenue|repo_name|lines_added|\n",
      "+--------------+----------+---------+------------------+-----------------+--------+--------------------+------------+-----------+---------+-----------+\n",
      "|            26|2023-03-08|     true|             366.0|65.34468841552734|    Alex|herself sip incep...|New Customer|  7167.0356|        M|         12|\n",
      "|            27|2023-12-14|     true|             388.0|85.50627899169922|    Alex|lute O'Brien coll...|     Support|  6864.0854|        Z|         90|\n",
      "|            28|2023-06-16|    false|             194.0|3.239738941192627|    Dale|spheroid wobble c...|     Support|  5590.8164|        S|         18|\n",
      "|            31|2023-06-06|    false|             112.0|37.53969192504883|  Sandra|tenure Courtney K...|New Customer|   2733.707|        B|         78|\n",
      "|            34|2023-12-01|     true|             342.0|85.51477813720703|  Sandra|iconoclast Bess m...|New Customer|  2914.0007|        V|         77|\n",
      "+--------------+----------+---------+------------------+-----------------+--------+--------------------+------------+-----------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = clean_data(exploded_df)\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+---------+------------------+---------+--------+------------------+----------+-----------+---------+-----------+\n",
      "|jira_ticket_id|date|completed|num_slack_messages|num_hours|engineer|ticket_description|initiative|new_revenue|repo_name|lines_added|\n",
      "+--------------+----+---------+------------------+---------+--------+------------------+----------+-----------+---------+-----------+\n",
      "|             0|   0|    14285|                 0|        0|       0|                 0|         0|          0|        0|          0|\n",
      "+--------------+----+---------+------------------+---------+--------+------------------+----------+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get the null distribution of the cleaned data\n",
    "analyze_null_distribution(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|completed|count|\n",
      "+---------+-----+\n",
      "|     NULL|14285|\n",
      "|     true|43052|\n",
      "|    false|42663|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#See the unique value count in the completed column\n",
    "cleaned_df.groupBy(\"completed\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data to parquet in the data folder\n",
    "cleaned_df.write.mode(\"overwrite\").parquet(\"../data/clean_data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
